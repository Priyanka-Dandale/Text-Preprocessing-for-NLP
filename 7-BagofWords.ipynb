{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words BoW\n",
    "\n",
    "**Bag of Words** is a technique for extracting features from text data for machine learning tasks, such as text classification and sentiment analysis. This is important because machine learning algorithms can’t process textual data. The process of converting the text to numbers is known as feature extraction or feature encoding.  \n",
    "\n",
    "A Bag of Words is based on the occurrence of words in a document. The process starts with finding the vocabulary in the text and measuring their occurrence. It is called a bag because the order and structure of words are not considered, just their occurrence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['python', 'is', 'amazing', 'and', 'fun'], ['python', 'is', 'not', 'just', 'fun', 'but', 'also', 'powerful'], ['learning', 'python', 'is', 'fun']]\n"
     ]
    }
   ],
   "source": [
    "## Step 1: Preprocessing the Text Data\n",
    "\n",
    "## We'll start by defining a simple function to process text, including tokenization, lowercasing, and removing punctuation.\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "# Sample text data: sentences\n",
    "corpus = [\n",
    "    \"Python is amazing and fun.\",\n",
    "    \"Python is not just fun but also powerful.\",\n",
    "    \"Learning Python is fun!\",\n",
    "]\n",
    "# Function to preprocess text\n",
    "def preprocess(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize: split the text into words\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the sample corpus\n",
    "processed_corpus = [preprocess(sentence) for sentence in corpus]\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['also', 'amazing', 'and', 'but', 'fun', 'is', 'just', 'learning', 'not', 'powerful', 'python']\n"
     ]
    }
   ],
   "source": [
    "## Step 2: Build Vocabulary\n",
    "\n",
    "## we need to scan through all the documents and build a complete list of unique words, that is our vocabulary\n",
    "\n",
    "# Initialize an empty set for the vocabulary\n",
    "vocabulary = set()\n",
    "\n",
    "# Build the vocabulary\n",
    "for sentence in processed_corpus:\n",
    "    vocabulary.update(sentence)\n",
    "\n",
    "# Convert to a sorted list\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "print(\"Vocabulary:\", vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Calculate Word Frequencies and Vectorize\n",
    "\n",
    "## We'll now calculate the frequency of each word in the vocabulary for every document in the processed corpus.\n",
    "\n",
    "def create_bow_vector(sentence, vocab):\n",
    "    vector = [0] * len(vocab)  # Initialize a vector of zeros\n",
    "    for word in sentence:\n",
    "        if word in vocab:\n",
    "            idx = vocab.index(word)  # Find the index of the word in the vocabulary\n",
    "            vector[idx] += 1  # Increment the count at that index\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vectors:\n",
      "['also', 'amazing', 'and', 'but', 'fun', 'is', 'just', 'learning', 'not', 'powerful', 'python']\n",
      "[0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1]\n",
      "[0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "## At this point, you will have created a Bag of Words representation for each document in your corpus.\n",
    "\n",
    "# Create BoW vector for each sentence in the processed corpus\n",
    "bow_vectors = [create_bow_vector(sentence, vocabulary) for sentence in processed_corpus]\n",
    "print(\"Bag of Words Vectors:\")\n",
    "print(vocabulary)\n",
    "for vector in bow_vectors:\n",
    "    print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Scikit-learn’s CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['also' 'amazing' 'and' 'but' 'fun' 'is' 'just' 'learning' 'not'\n",
      " 'powerful' 'python']\n",
      "BoW Representation:\n",
      "[[0 1 1 0 1 1 0 0 0 0 1]\n",
      " [1 0 0 1 1 1 1 0 1 1 1]\n",
      " [0 0 0 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Original corpus\n",
    "corpus = [\n",
    "    \"Python is amazing and fun.\",\n",
    "    \"Python is not just fun but also powerful.\",\n",
    "    \"Learning Python is fun!\",\n",
    "]\n",
    "\n",
    "# Create a CountVectorizer Object\n",
    "vectorizer = CountVectorizer()  # binary=True for binaryBoW and max_features = 10, to select top 10 features based on frequency\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the generated vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the Bag-of-Words matrix\n",
    "print(\"BoW Representation:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['but' 'fantastic' 'great' 'hated' 'it' 'loved' 'movie' 'not' 'okay'\n",
      " 'terrible' 'the' 'was']\n",
      "Document-Term Matrix:\n",
      " [[0 1 0 0 1 1 1 0 0 0 1 1]\n",
      " [1 0 1 0 0 0 1 1 1 0 1 1]\n",
      " [0 0 0 1 1 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Sample corpus of movie reviews\n",
    "corpus = [\n",
    "    \"I loved the movie, it was fantastic!\",\n",
    "    \"The movie was okay, but not great.\",\n",
    "    \"I hated the movie, it was terrible.\",\n",
    "]\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to a document-term matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the document-term matrix into a dense format (optional for visualization)\n",
    "X_dense = X.toarray()\n",
    "\n",
    "# Get the vocabulary (mapping of words to index positions)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the vocabulary and document-term matrix\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Document-Term Matrix:\\n\", X_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nltk_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
